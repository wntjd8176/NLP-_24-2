{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "732df096",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "import re\n",
    "import faiss\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9722d3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-gpu\n",
      "  Downloading faiss_gpu-1.7.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 85.5 MB 125.1 MB/s eta 0:00:01  |█▍                              | 3.8 MB 6.9 MB/s eta 0:00:12\n",
      "\u001b[?25hInstalling collected packages: faiss-gpu\n",
      "Successfully installed faiss-gpu-1.7.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ab8e70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.8.0.post1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 27.0 MB 7.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from faiss-cpu) (1.23.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from faiss-cpu) (23.1)\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.8.0.post1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6a43863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model. /home/u4026/CBNU_KoSentenceBERT_SKT/.cache/kobert_v1.zip\n",
      "using cached model. /home/u4026/CBNU_KoSentenceBERT_SKT/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n",
      "using cached model. /home/u4026/CBNU_KoSentenceBERT_SKT/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n",
      "Load Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u4026/.local/lib/python3.8/site-packages/sentence_transformers/models/Transformer.py:41: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.auto_model.load_state_dict(torch.load(model_name_or_path+'/result.pt'))\n"
     ]
    }
   ],
   "source": [
    "model_path = '/home/u4026/KoSentenceBERT_SKTBERT/output/training_sts'\n",
    "model = SentenceTransformer(model_path)\n",
    "#model = SentenceTransformer(\"distiluse-base-multilingual-cased-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5901a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformers\n",
      "Version: 4.46.3\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: /home/u4026/.local/lib/python3.8/site-packages\n",
      "Requires: regex, filelock, pyyaml, tokenizers, huggingface-hub, safetensors, requests, numpy, tqdm, packaging\n",
      "Required-by: sentence-transformers, kobert\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip show transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d41cb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV 파일 로드\n",
    "csv_file = \"books_summary.csv\"\n",
    "data = pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a532ff1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_filter_sentences(text):               #무시하자\n",
    "    \"\"\"\n",
    "    텍스트를 KSS와 추가 기준으로 분리하고, 의미 없는 연결어를 제거\n",
    "    \"\"\"\n",
    "    # KSS로 1차 분리\n",
    "    sentences = kss.split_sentences(text)\n",
    "    \n",
    "    # 추가 기준 적용 ('그리고', '또', ',' 기준으로 분리)\n",
    "    split_sentences = []\n",
    "    for sentence in sentences:\n",
    "        parts = re.split(r'(그리고|또|,)', sentence)\n",
    "        parts = [p.strip() for p in parts if p.strip()]  # 공백 제거 및 빈 문자열 제거\n",
    "        split_sentences.extend(parts)\n",
    "    \n",
    "    # 불필요한 연결어 필터링\n",
    "    filtered_sentences = [s for s in split_sentences if s not in [\"그리고\", \"또\", \",\"]]\n",
    "    \n",
    "    return filtered_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7937482a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_embedding(sentences, model):        #무시하자\n",
    "    \"\"\"\n",
    "    여러 문장의 임베딩 값을 평균화\n",
    "    \"\"\"\n",
    "    if not sentences:\n",
    "        return None  # 빈 입력에 대한 처리\n",
    "\n",
    "    embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "    avg_embedding = embeddings.mean(dim=0)  # 평균 계산\n",
    "    return avg_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ffbcc847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리된 텍스트 열 추출\n",
    "corpus = data['내용요약'].tolist()\n",
    "titles = data['첵제목'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b086534f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Oh! You have mecab in your environment. Kss will take this as a backend! :D\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 코퍼스 문장 분리 및 임베딩 평균 계산                무시하자\n",
    "corpus_embeddings = []\n",
    "for doc in corpus:\n",
    "    sentences = split_and_filter_sentences(doc)\n",
    "    avg_embedding = get_average_embedding(sentences, model)\n",
    "    corpus_embeddings.append(avg_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5fef98af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STS 코퍼스 임베딩 생성\n",
    "#sts_corpus_embeddings = model.encode(corpus, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df2a06e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dimension = corpus_embeddings.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b37484fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sts_embedding_dimension = sts_corpus_embeddings.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44a407a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"상위 5개 '내용요약' 데이터:\")\n",
    "print(corpus[:5]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1c5f19af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAISS CPU 설정\n",
    "index = faiss.IndexFlatL2(embedding_dimension)  # L2 거리 기반 인덱스 생성\n",
    "index.add(sts_corpus_embeddings.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c05a436e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 후보군 찾기 (KNN 활용)\n",
    "def find_candidates_with_knn(query, corpus, titles, n_neighbors=3):\n",
    "    # STS 임베딩 생성\n",
    "    corpus_embeddings = model.encode(corpus)\n",
    "    query_embedding = model.encode([query])\n",
    "\n",
    "    # Nearest Neighbors 모델 초기화 및 학습\n",
    "    knn = NearestNeighbors(n_neighbors=n_neighbors, metric='cosine')\n",
    "    knn.fit(corpus_embeddings)\n",
    "\n",
    "    # Query와 가장 가까운 이웃 찾기\n",
    "    distances, indices = knn.kneighbors(query_embedding)\n",
    "\n",
    "    candidates = []\n",
    "    for dist, idx in zip(distances[0], indices[0]):\n",
    "        candidates.append({\n",
    "            \"책제목\": titles[idx],\n",
    "            \"내용요약\": corpus[idx],\n",
    "            \"유사도\": 1 - dist  # 코사인 거리 -> 유사도로 변환\n",
    "        })\n",
    "\n",
    "    # 유사도를 기준으로 정렬\n",
    "    return sorted(candidates, key=lambda x: x[\"유사도\"], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "283a32d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sts를 활용해 포함 관계 분석\n",
    "def find_entailments_with_batches(query, corpus, titles, batch_size=500, top_k=3, threshold=0.2):\n",
    "    results = []\n",
    "    query_embedding = model.encode([query], convert_to_tensor=True)\n",
    "\n",
    "    for i in range(0, len(corpus), batch_size):\n",
    "        batch_corpus = corpus[i:i + batch_size]\n",
    "        batch_titles = titles[i:i + batch_size]\n",
    "        batch_embeddings = model.encode(batch_corpus, convert_to_tensor=True)\n",
    "\n",
    "        # 코사인 유사도 계산 (NLI 모델 활용)\n",
    "        similarities = util.pytorch_cos_sim(query_embedding, batch_embeddings)\n",
    "\n",
    "        for j, similarity in enumerate(similarities[0]):\n",
    "            score = similarity.item()\n",
    "            if score >= threshold:  # Threshold 적용\n",
    "                results.append({\n",
    "                    \"책제목\": batch_titles[j],\n",
    "                    \"내용요약\": batch_corpus[j],\n",
    "                    \"포함확률\": score\n",
    "                })\n",
    "\n",
    "    # 상위 N개 결과 정렬\n",
    "    return sorted(results, key=lambda x: x[\"포함확률\"], reverse=True)[:top_k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "16274c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#faiss 써서 포함관계분석\n",
    "def find_entailments_with_faiss(query, corpus, titles, top_k=3, threshold=1.0):\n",
    "    results = []\n",
    "    query_embedding = model.encode([query], convert_to_tensor=True).cpu().numpy()  # GPU 텐서를 CPU NumPy 배열로 변환\n",
    "\n",
    "    # FAISS에서 최근접 이웃 검색\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "\n",
    "    for dist, idx in zip(distances[0], indices[0]):\n",
    "        score = 1 - dist  # 거리 값을 유사도로 변환\n",
    "        if score <=threshold:  # Threshold 적용\n",
    "            results.append({\n",
    "                \"책제목\": titles[idx],\n",
    "                \"내용요약\": corpus[idx],\n",
    "                 \"포함확률\": 1 - dist if dist < 1 else 0,  # 거리 값을 유사도로 변환\n",
    "                \"L2 거리\": dist\n",
    "                #\"포함확률\": score\n",
    "            })\n",
    "\n",
    "    return sorted(results, key=lambda x: x[\"포함확률\"], reverse=True)[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3594e18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLI모델로 평가\n",
    "def evaluate_with_nli(query, entailment_results):\n",
    "    query_embedding = nli_model.encode([query], convert_to_tensor=True)\n",
    "    results = []\n",
    "\n",
    "    for result in entailment_results:\n",
    "        corpus_embedding = nli_model.encode([result['내용요약']], convert_to_tensor=True)\n",
    "        similarity = util.pytorch_cos_sim(query_embedding, corpus_embedding).item()\n",
    "        results.append({\n",
    "            \"책제목\": result['책제목'],\n",
    "            \"내용요약\": result['내용요약'],\n",
    "            \"포함확률\": result['포함확률'],\n",
    "            \"유사도\": similarity\n",
    "        })\n",
    "\n",
    "    # 유사도를 기준으로 정렬\n",
    "    return sorted(results, key=lambda x: x[\"유사도\"], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75f1a76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STS 모델로 평가\n",
    "def evaluate_with_sts(query, entailment_results):\n",
    "    query_embedding = model.encode([query], convert_to_tensor=True)\n",
    "    results = []\n",
    "\n",
    "    for result in entailment_results:\n",
    "        corpus_embedding = model.encode([result['내용요약']], convert_to_tensor=True)\n",
    "        similarity = util.pytorch_cos_sim(query_embedding, corpus_embedding).item()\n",
    "        results.append({\n",
    "            \"책제목\": result['책제목'],\n",
    "            \"내용요약\": result['내용요약'],\n",
    "            \"포함확률\": result['포함확률'],\n",
    "            \"유사도\": similarity\n",
    "        })\n",
    "\n",
    "    # 유사도를 기준으로 정렬\n",
    "    return sorted(results, key=lambda x: x[\"유사도\"], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "374dd598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: 쿼리 입력 및 상위 3개 검색           이거안씀\n",
    "def search_nearest_neighbors(query, top_k=3):\n",
    "    query_embedding = model.encode([query], convert_to_tensor=True).cpu().numpy()  # GPU 텐서를 NumPy 배열로 변환\n",
    "    \n",
    "    # FAISS에서 상위 3개 검색\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    \n",
    "    # 검색된 3개의 데이터 반환\n",
    "    nearest_data = [{\"index\": idx, \"내용요약\": corpus[idx], \"distance\": dist}\n",
    "                    for dist, idx in zip(distances[0], indices[0])]\n",
    "    return nearest_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8738675a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: 검색된 데이터와 쿼리 상세 비교       이거 안씀\n",
    "def compare_with_nearest(query, nearest_data):\n",
    "    query_embedding = model.encode([query], convert_to_tensor=True).cpu()  # GPU 텐서를 CPU 텐서로 변환\n",
    "    \n",
    "    # 검색된 데이터 임베딩 추출\n",
    "    selected_embeddings = np.array([corpus_embeddings[entry[\"index\"]].cpu().numpy() for entry in nearest_data])\n",
    "    \n",
    "    # 유사도 계산 (코사인 유사도)\n",
    "    similarities = util.pytorch_cos_sim(query_embedding, selected_embeddings)\n",
    "    \n",
    "    # 유사도 추가\n",
    "    for i, entry in enumerate(nearest_data):\n",
    "        entry[\"유사도\"] = similarities[0][i].item()\n",
    "    return sorted(nearest_data, key=lambda x: x[\"유사도\"], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dee55297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 유사도 평가 (STS 활용)\n",
    "def evaluate_with_stsk(query, candidates):\n",
    "    query_embedding = model.encode([query], convert_to_tensor=True)\n",
    "    results = []\n",
    "\n",
    "    for candidate in candidates:\n",
    "        corpus_embedding = model.encode([candidate['내용요약']], convert_to_tensor=True)\n",
    "        similarity = util.pytorch_cos_sim(query_embedding, corpus_embedding).item()\n",
    "\n",
    "        results.append({\n",
    "            \"책제목\": candidate['책제목'],\n",
    "            \"내용요약\": candidate['내용요약'],\n",
    "            \"유사도\": similarity\n",
    "        })\n",
    "\n",
    "    # 유사도를 기준으로 정렬\n",
    "    return sorted(results, key=lambda x: x[\"유사도\"], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a5a1e52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "텍스트를 입력하세요: 새 교육과정과 관련된 책 추천해줘\n"
     ]
    }
   ],
   "source": [
    "# 사용자가 입력한 텍스트\n",
    "query = input(\"텍스트를 입력하세요: \")\n",
    "\n",
    "\n",
    "# 쿼리 임베딩 생성\n",
    "#query_embedding = nli_model.encode([query], convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e9f6cbe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "가장 관련성 높은 책 추천:\n",
      "책제목: 교육광장 2016 겨울호 (Vol. 62)\n",
      "내용요약: 새 교육과정에 따른 평가기준의 개발이 필요해서, 한국교육과정평가원에서는 총론과 교과별 평가기준 등을 연구 개발 하였다.  핵심 개발 방향은 2015년 개정 교육과정이 학교현장에서 운영되도록 하는 것이다.\n",
      "유사도: 0.5778\n",
      "\n",
      "책제목: 교육정책포럼 2015년 07월 (통권 265호)\n",
      "내용요약: 개정 교육과정의 운영 방법을 명확히 하기 위해 전반적으로 학생들의 학습 강점과 요구사항 등을 교육과정에 적절히 반영하였다.\n",
      "유사도: 0.5211\n",
      "\n",
      "책제목: 미래 사회 교육 환경 변화에 따른 교과서 발전 방안\n",
      "내용요약: 함수곤은 전통적인 학습 교재로서의 교과서의 역할을 다섯 가지 제시했다. 또한 학습 과제의 효율적인 제시, 학습의 개성화 기능, 정착 기능 등을 최근 등장한 교과서의 학습재로서의 역할을 위해 필요하다고 설명했다.\n",
      "유사도: 0.4998\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NLI를 사용해 상위 3개 필터링\n",
    "#entailment_results = find_entailments_with_batches(query, corpus, titles, batch_size=500, top_k=3)\n",
    "#knn\n",
    "candidates= find_candidates_with_knn(query, corpus, titles, n_neighbors=3)\n",
    "\n",
    "# faiss를 사용해 상위 3개 필터링\n",
    "#entailment_results = find_entailments_with_faiss(query,corpus,titles, top_k=3)\n",
    "\n",
    "# NLI로 유사도 평가\n",
    "#final_results = evaluate_with_nli(query, entailment_results)\n",
    "\n",
    "#STS로 유사도 평가\n",
    "final_results = evaluate_with_stsk(query, entailment_results)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"\\n가장 관련성 높은 책 추천:\")\n",
    "for result in final_results:\n",
    "    print(f\"책제목: {result['책제목']}\")\n",
    "    print(f\"내용요약: {result['내용요약']}\")\n",
    "   # print(f\"포함확률: {result['포함확률']:.4f}\")\n",
    "    print(f\"유사도: {result['유사도']:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2eb7fd86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "가장 유사한 3개의 문장:\n",
      "Rank 1:\n",
      "내용요약: 연구반은 지역 MBC, KBS 지역총국의 편성‧경영 현황을 검토하고 관계자의 자체제작물 편성의무 의견과 논의를 통해 제도적 실현가능성과 세부사항을 논의했다. \n",
      "유사도: 0.5576\n",
      "\n",
      "Rank 2:\n",
      "내용요약: PD는 자격유지를 위해 기획재정부 고시 규정상 의무를 이행해야하는 바, 이행 실적에 따라 점수로 평가를 받게되고, PPD는 특성상 보다 적은 의무를 부과한다.\n",
      "유사도: 0.5176\n",
      "\n",
      "Rank 3:\n",
      "내용요약: 한국산업기술평가관리원은 국가 혁신역량제고를 목적으로 설립되었다. 기관장은 설립목적에 부합하는 다양한 경영방침을 수립하고 신핵심가치 설정과 이를 실행하기 위한 내부 실행력을 강화하고 공공기관의 사회적 책임을 다하기 위해 노력하는 등의 모습을 보였다.\n",
      "유사도: 0.4974\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 함수 호출           이거 안씀\n",
    "nearest_data = search_nearest_neighbors(query, top_k=3)\n",
    "final_results = compare_with_nearest(query, nearest_data)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"\\n가장 유사한 3개의 문장:\")\n",
    "for idx, result in enumerate(final_results):\n",
    "    print(f\"Rank {idx + 1}:\")\n",
    "    print(f\"내용요약: {result['내용요약']}\")\n",
    "    print(f\"유사도: {result['유사도']:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "20e137bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: 요즘 우리나라 대통령이 개엄령을 일으켜서 놀라웠는데 예전에 있었던 광주시위에 관한책을 소개시켜줘 광주시위후 슬픔을 받는 사람들의 이야기였으면 해\n",
      "\n",
      "Top 3 most similar sentences in corpus:\n",
      "소설은 6개의 장으로 나누어져 있었는데요 각 장마다 다른 인물의 시선으로 이야기가 진행되고 있었어요 주인공들은 광주에서 일어난 시위 사건 속에 있던 사람들로 각자의 슬픔을 안고 살아가고 있습니다 (Score: 0.4742)\n",
      "소설은 6개의 장으로 나누어져 있었는데요 각 장마다 다른 인물의 시선으로 이야기가 진행되고 있었어요 주인공들은 광주에서 일어난 시위 사건 속에 있던 사람들로 각자의 슬픔을 안고 살아가고 있습니다 (Score: 0.2166)\n",
      "소설은 6개의 장으로 나누어져 있었는데요 각 장마다 다른 인물의 시선으로 이야기가 진행되고 있었어요 주인공들은 광주에서 일어난 시위 사건 속에 있던 사람들로 각자의 슬픔을 안고 살아가고 있습니다 (Score: 0.1448)\n"
     ]
    }
   ],
   "source": [
    "# 코사인 유사도 비교         #이거 안씀\n",
    "if query_embedding is not None:\n",
    "    cos_scores = [util.pytorch_cos_sim(query_embedding, corpus_embedding)[0].item() for corpus_embedding in corpus_embeddings]\n",
    "\n",
    "    # 상위 유사도 결과 출력\n",
    "    top_k = min(5, len(cos_scores))\n",
    "    top_results = np.argsort(cos_scores)[-top_k:][::-1]  # 유사도 높은 순으로 정렬\n",
    "\n",
    "    print(\"\\n\\n======================\\n\\n\")\n",
    "    print(\"Query:\", query)\n",
    "    print(\"\\nTop {} most similar sentences in corpus:\".format(top_k))\n",
    "\n",
    "    for idx in top_results:\n",
    "        print(f\"{corpus[0]} (Score: {cos_scores[idx]:.4f})\")\n",
    "else:\n",
    "    print(\"입력 텍스트에서 유효한 문장이 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8829e794",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'util' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 코사인 유사도 계산\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m cos_scores \u001b[38;5;241m=\u001b[39m \u001b[43mutil\u001b[49m\u001b[38;5;241m.\u001b[39mpytorch_cos_sim(query_embedding, corpus_embeddings)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      3\u001b[0m cos_scores \u001b[38;5;241m=\u001b[39m cos_scores\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# 책 제목별 최고 유사도 선택\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'util' is not defined"
     ]
    }
   ],
   "source": [
    "# 코사인 유사도 계산\n",
    "cos_scores = util.pytorch_cos_sim(query_embedding, corpus_embeddings)[0]\n",
    "cos_scores = cos_scores.cpu()\n",
    "\n",
    "# 책 제목별 최고 유사도 선택\n",
    "best_results = {}\n",
    "for idx, score in enumerate(cos_scores):\n",
    "    title = titles[idx]\n",
    "    sentence = corpus[idx].strip()\n",
    "    if title not in best_results or best_results[title]['score'] < score.item():\n",
    "        best_results[title] = {'sentence': sentence, 'score': score.item()}\n",
    "\n",
    "    # 유사도 점수가 높은 순으로 정렬\n",
    "sorted_results = sorted(best_results.items(), key=lambda x: x[1]['score'], reverse=True)\n",
    "\n",
    "# 가장 유사한 결과 가져오기\n",
    "#top_k = 3\n",
    "#top_results = np.argpartition(-cos_scores, range(top_k))[0:top_k]\n",
    "\n",
    "# 결과 출력\n",
    "print(\"\\n\\n======================\\n\\n\")\n",
    "print(\"Query:\", query)\n",
    "print(\"\\nTop 5 most similar sentences in corpus:\")\n",
    "for title, result in sorted_results[:top_k]:\n",
    "        print(f\"Title: {title}, Sentence: {result['sentence']} (Score: {result['score']:.4f})\")\n",
    "\n",
    "#for idx in top_results[0:top_k]:\n",
    "    #print(f\"Title: {titles[idx]}, Sentence: {corpus[idx].strip()} (Score: {cos_scores[idx]:.4f})\")\n",
    "    #print(f\"{corpus[idx].strip()} (Score: {cos_scores[idx]:.4f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
